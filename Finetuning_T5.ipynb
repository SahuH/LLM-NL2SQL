{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelWithLMHead, AutoTokenizer\n",
    "from torch.utils.data import Dataset\n",
    "from datasets import load_dataset\n",
    "import nltk\n",
    "import sentencepiece\n",
    "import json\n",
    "\n",
    "import random, warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"t5-base\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset spider (/Users/hsahu/.cache/huggingface/datasets/spider/spider/1.0.0/4e5143d825a3895451569c8b9b55432b91a4bc2d04d390376c950837f4680daa)\n",
      "100%|██████████| 2/2 [00:00<00:00, 124.04it/s]\n"
     ]
    }
   ],
   "source": [
    "spider_dataset = load_dataset('spider')  # Using spider dataset from huggingface datasets library (and not the one stored in local)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SpiderDataset(Dataset):\n",
    "    def __init__(self, data, tokenizer, max_length):\n",
    "        self.data = data\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        db_id = self.data[idx]['db_id']\n",
    "        schema = self.get_schema(db_id)\n",
    "\n",
    "        input_text = f\"translate English to SQL: {self.data[idx]['question']} <schema> {schema} </s>\"\n",
    "        target_text = self.data[idx]['query']\n",
    "        encoding = self.tokenizer(input_text, max_length=self.max_length, truncation=True, padding='max_length', return_tensors='pt')\n",
    "        target = self.tokenizer(target_text, max_length=self.max_length, truncation=True, padding='max_length', return_tensors='pt')\n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten(),\n",
    "            'labels': target['input_ids'].flatten(),\n",
    "            'data_item': self.data[idx] \n",
    "        }\n",
    "\n",
    "    def get_schema(self, db_id):\n",
    "        schema_data = next(item for item in tables_data if item['db_id'] == db_id)\n",
    "        schema = \" \".join([f\"Table: {table_name} Columns: {', '.join([col_name for _, col_name in schema_data['column_names'] if schema_data['table_names'][table_idx] == table_name])}\" for table_idx, table_name in enumerate(schema_data['table_names'])])\n",
    "        return schema\n",
    "\n",
    "# Load tables.json\n",
    "with open('./spider_dataset/tables.json', 'r') as f:\n",
    "    tables_data = json.load(f)\n",
    "\n",
    "train_dataset = SpiderDataset(spider_dataset['train'], tokenizer, max_length=128)\n",
    "val_dataset = SpiderDataset(spider_dataset['validation'], tokenizer, max_length=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./output',\n",
    "    num_train_epochs=5,\n",
    "    learning_rate=1e-6,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=64,\n",
    "    warmup_steps=500,\n",
    "    weight_decay=0.01,\n",
    "    save_strategy='epoch',\n",
    "    evaluation_strategy='epoch',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelWithLMHead.from_pretrained(\"t5-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset \n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sql(query):\n",
    "    \n",
    "    input_text = \"translate English to SQL: %s </s>\" % query\n",
    "    \n",
    "    features = tokenizer([input_text], return_tensors='pt')\n",
    "\n",
    "    output = model.generate(input_ids=features['input_ids'], \n",
    "               attention_mask=features['attention_mask'])\n",
    "\n",
    "    return tokenizer.decode(output[0])\n",
    "\n",
    "def get_sql_with_schema(query, schema):\n",
    "    # Concatenate the schema information with the input query text, separated by a special token, such as `<schema>`\n",
    "    input_text = f\"translate English to SQL: {query} <schema> {schema} </s>\"\n",
    "\n",
    "    features = tokenizer([input_text], return_tensors='pt')\n",
    "\n",
    "    output = model.generate(input_ids=features['input_ids'], \n",
    "                             attention_mask=features['attention_mask'])\n",
    "\n",
    "    decoded_output = tokenizer.decode(output[0])\n",
    "\n",
    "    # Remove the <pad> token from the output\n",
    "    return decoded_output.replace('<pad>', '').strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gold_file = open('gold.txt', 'w')\n",
    "pred_file = open('pred.txt', 'w')\n",
    "\n",
    "num_queries = 5\n",
    "count = 0\n",
    "for idx in random.sample(range(len(val_dataset)), num_queries):\n",
    "    item = val_dataset[idx]\n",
    "    data_item = item['data_item']\n",
    "    print(f'{count + 1}/{num_queries}')\n",
    "    print(f\"Text: {data_item['question']}\")\n",
    "\n",
    "    # Get schema information\n",
    "    db_id = data_item['db_id']\n",
    "    schema = val_dataset.get_schema(db_id)\n",
    "\n",
    "    pred = get_sql_with_schema(data_item['question'], schema)\n",
    "    gold = data_item['query']\n",
    "\n",
    "    gold_file.write(gold + '\\t' + db_id + '\\n')\n",
    "    pred_file.write(pred + '\\n')\n",
    "    \n",
    "    print(f\"Pred SQL: {pred}\")\n",
    "    print(f\"True SQL: {gold}\\n\")\n",
    "\n",
    "    count += 1\n",
    "  \n",
    "gold_file.close()\n",
    "pred_file.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using repo for evaluation: https://github.com/taoyds/test-suite-sql-eval.git\n",
    "\n",
    "!git clone https://github.com/taoyds/test-suite-sql-eval.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python test-suite-sql-eval/evaluation.py --gold gold.txt --pred pred.txt --db spider_dataset/database --table spider_dataset/tables.json --etype all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import argparse\n",
    "# import pytorch_lightning as pl\n",
    "# import torch\n",
    "# from torch.utils.data import DataLoader\n",
    "\n",
    "# from transformers import (\n",
    "#     AdamW,\n",
    "#     AutoConfig,\n",
    "#     AutoModelWithLMHead,\n",
    "#     AutoTokenizer,\n",
    "#     get_linear_schedule_with_warmup,\n",
    "# )\n",
    "\n",
    "\n",
    "# class customT5(pl.LightningModule):\n",
    "#     def __init__(self, **config_kwargs):\n",
    "\n",
    "#         super().__init__()\n",
    "#         self.config = AutoConfig.from_pretrained(\"t5-base\")\n",
    "#         self.tokenizer = AutoTokenizer.from_pretrained(\"t5-base\")\n",
    "#         self.model = AutoModelWithLMHead.from_pretrained(\"t5-base\", config=self.config)\n",
    "\n",
    "#         self.dataset_kwargs: dict = dict(\n",
    "#             data_dir=self.hparams.data_dir,\n",
    "#             max_source_length=self.hparams.max_source_length,\n",
    "#             max_target_length=self.hparams.max_target_length,\n",
    "#         )\n",
    "\n",
    "#     def forward(self, input_ids,  attention_mask=None, decoder_input_ids=None, lm_labels=None):\n",
    "\n",
    "#         return self.model( input_ids,\n",
    "#             attention_mask=attention_mask,\n",
    "#             decoder_input_ids=decoder_input_ids,\n",
    "#             lm_labels=lm_labels,\n",
    "#         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def generic_train(model: customT5, output_dir):\n",
    "\n",
    "#     checkpoint_callback = pl.callbacks.ModelCheckpoint(\n",
    "#         filepath=output_dir, prefix=\"checkpoint\", monitor=\"val_loss\", mode=\"min\", save_top_k=5\n",
    "#     )\n",
    "\n",
    "#     train_params = dict(\n",
    "#         accumulate_grad_batches=args.gradient_accumulation_steps,\n",
    "#         max_epochs=10,\n",
    "#         early_stop_callback=False,\n",
    "#         checkpoint_callback=checkpoint_callback\n",
    "#     )\n",
    "\n",
    "#     trainer = pl.Trainer(**train_params)\n",
    "\n",
    "#     trainer.fit(model)\n",
    "\n",
    "#     return trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# output_dir = \"model_output\"\n",
    "# model = customT5()\n",
    "# trainer = generic_train(model)\n",
    "\n",
    "# model.model.save_pretrained(output_dir)\n",
    "# model.tokenizer.save_pretrained(output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.13 ('tf')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "66b1c3ef371cb3a1b7ba076fa8b8f7fa1fc18dc5ad8f399afa7dd772ee14fb28"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
