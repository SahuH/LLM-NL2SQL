{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelWithLMHead, AutoTokenizer\n",
    "from torch.utils.data import Dataset\n",
    "from datasets import load_dataset\n",
    "import nltk\n",
    "import json\n",
    "\n",
    "import random, warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"t5-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset spider (/Users/hsahu/.cache/huggingface/datasets/spider/spider/1.0.0/4e5143d825a3895451569c8b9b55432b91a4bc2d04d390376c950837f4680daa)\n",
      "100%|██████████| 2/2 [00:00<00:00, 63.29it/s]\n"
     ]
    }
   ],
   "source": [
    "spider_dataset = load_dataset('spider')  # Using spider dataset from huggingface datasets library (and not the one stored in local)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SpiderDataset(Dataset):\n",
    "    def __init__(self, data, tokenizer, max_length):\n",
    "        self.data = data\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        db_id = self.data[idx]['db_id']\n",
    "        schema = self.get_schema(db_id)\n",
    "\n",
    "        input_text = f\"translate English to SQL: {self.data[idx]['question']} <schema> {schema} </s>\"\n",
    "        target_text = self.data[idx]['query']\n",
    "        encoding = self.tokenizer(input_text, max_length=self.max_length, truncation=True, padding='max_length', return_tensors='pt')\n",
    "        target = self.tokenizer(target_text, max_length=self.max_length, truncation=True, padding='max_length', return_tensors='pt')\n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten(),\n",
    "            'labels': target['input_ids'].flatten(),\n",
    "            'data_item': self.data[idx] \n",
    "        }\n",
    "\n",
    "    def get_schema(self, db_id):\n",
    "        schema_data = next(item for item in tables_data if item['db_id'] == db_id)\n",
    "        schema = \" \".join([f\"Table: {table_name} Columns: {', '.join([col_name for _, col_name in schema_data['column_names'] if schema_data['table_names'][table_idx] == table_name])}\" for table_idx, table_name in enumerate(schema_data['table_names'])])\n",
    "        return schema\n",
    "\n",
    "# Load tables.json\n",
    "with open('./spider_dataset/tables.json', 'r') as f:\n",
    "    tables_data = json.load(f)\n",
    "\n",
    "train_dataset = SpiderDataset(spider_dataset['train'], tokenizer, max_length=128)\n",
    "val_dataset = SpiderDataset(spider_dataset['validation'], tokenizer, max_length=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=64,\n",
    "    warmup_steps=500,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir='./logs',\n",
    "    logging_steps=10,\n",
    "    save_strategy='epoch',\n",
    "    evaluation_strategy='epoch',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelWithLMHead.from_pretrained(\"t5-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset \n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running training *****\n",
      "  Num examples = 7000\n",
      "  Num Epochs = 3\n",
      "  Instantaneous batch size per device = 16\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 1314\n",
      "  Number of trainable parameters = 222903552\n",
      "  0%|          | 0/1314 [00:00<?, ?it/s]The following columns in the training set don't have a corresponding argument in `T5ForConditionalGeneration.forward` and have been ignored: data_item. If data_item are not expected by `T5ForConditionalGeneration.forward`,  you can safely ignore this message.\n"
     ]
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.13 ('tf')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "66b1c3ef371cb3a1b7ba076fa8b8f7fa1fc18dc5ad8f399afa7dd772ee14fb28"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
